{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Demo] Working with PSANA inside Jupyter lab notebook\n",
    "\n",
    "This notebook starts a Dask scheduler that serves as a bridge to share data between PSANA and Jupyter Lab. Using the `slurm` package, users can submit a PSANA job and return results to be manipulated within the notebook. \n",
    "\n",
    "This notebook is able to run the example shown in https://confluence.slac.stanford.edu/display/PSDMInternal/Small+H5\n",
    "\n",
    "Note: `slurm_magic` version from https://github.com/muammar/slurm-magic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the python file used to run the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.py\n",
    "from psana import DataSource\n",
    "import numpy as np\n",
    "import sys\n",
    "from dask.distributed import Client, Queue\n",
    "\n",
    "# The code below fail with MPI rank errors. \n",
    "# import os\n",
    "# os.environ['PS_SRV_NODES'] = \"2\"\n",
    "\n",
    "# Connect to the Dask scheduler\n",
    "client = Client(scheduler_file=\"scheduler.json\")\n",
    "\n",
    "# Create a Queue object\n",
    "queue = Queue(\"psana\")\n",
    "\n",
    "# called back on each SRV node, for every smd.event() call below\n",
    "def test_callback(data_dict):\n",
    "    print(data_dict)\n",
    "\n",
    "ds = DataSource(exp='xpptut13', run=1, dir='.tmp')\n",
    "\n",
    "# batch_size here specifies how often the dictionary of information\n",
    "# is sent to the SRV nodes\n",
    "smd = ds.smalldata(filename='my.h5', batch_size=5, callbacks=[test_callback])\n",
    "run = next(ds.runs())\n",
    "\n",
    "# necessary (instead of \"None\") since some ranks may not receive events\n",
    "# and the smd.sum() below could fail\n",
    "\n",
    "arrsum = np.zeros((2), dtype=np.int)\n",
    "\n",
    "for i,evt in enumerate(run.events()):\n",
    "    myones = np.ones_like(arrsum)\n",
    "    smd.event(evt, myfloat=2.0, arrint=myones)\n",
    "    arrsum += myones\n",
    "    queue.put(arrsum.tolist())\n",
    "\n",
    "\n",
    "# This fails as reported in https://github.com/slac-lcls/lcls2/issues/11\n",
    "\n",
    "# if smd.summary:\n",
    "#     smd.sum(arrsum)\n",
    "#     smd.save_summary({'summary_array' : arrsum}, summary_int=1)\n",
    "# smd.done()\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.sub\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.sub\n",
    "#!/bin/bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 3\n",
    "#SBATCH -c 1\n",
    "#SBATCH -C haswell\n",
    "#SBATCH -q debug\n",
    "#SBATCH -J smallD\n",
    "#SBATCH --account=m3384\n",
    "#SBATCH --mail-user=melkhatibr@lbl.gov\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -t 00:30:00\n",
    "\n",
    "export PS_SRV_NODES=2\n",
    "\n",
    "srun -n 6 -o example.log python example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"remove # to clean\")\n",
    "!rm -rf *.json *.log .tmp *.out dask-worker-space/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `slurm_magic` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext slurm_magic\n",
    "%squeue -u melkhati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to start a scheduler in the login node to be our bridge to share data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = 'scheduler.json'\n",
    "cluster = LocalCluster(n_workers=1, threads_per_worker=1, host=\"0.0.0.0\")\n",
    "client = Client(cluster)\n",
    "client.write_scheduler_file(scheduler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify our scheduler is up and running. We also created a `scheduler.json` file that can be used to connect as many notebooks as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "if os.path.isfile(scheduler_file):\n",
    "    print(\"Scheduler file with name {} exists!\".format(scheduler_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that client is up and running, there are potentially two ways to submit and gather data from a PSANA job:\n",
    "\n",
    "1. Using a `Queue` in Dask. \n",
    "2. Using Pub/Sub scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /global/common/software/lcls/psana/lcls2/psana/psana/tests/setup_input_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env PS_SRV_NODES=2\n",
    "# %srun -n 3 -A m3384  -C haswell -J smallD -q debug  -o example.log python example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sbatch example.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%squeue -u melkhati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue(\"psana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "for data in range(q.qsize()):\n",
    "    results.append(np.array(q.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(results)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
